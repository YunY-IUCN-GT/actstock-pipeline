"""
Airflow DAG: Monthly Portfolio Rebalance
=========================================
ì‹¤í–‰ ìŠ¤ì¼€ì¤„: ë§¤ì£¼ ì¼ìš”ì¼ 14:00 UTC (23:00 KST) - ë§ˆì§€ë§‰ ì¼ìš”ì¼ë§Œ ì‹¤í–‰
ëª©ì : 5ì¼/10ì¼/20ì¼ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í†µí•©í•˜ì—¬ ë‹¤ìŒ 20ì˜ì—…ì¼ ë™ì•ˆ ìœ ì§€í•  ìµœì¢… ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„±

Output:
- 08_analytics_monthly_portfolio í…Œì´ë¸”
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess
import logging

logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def is_last_sunday_of_month(**context):
    """
    ì˜¤ëŠ˜ì´ í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì¸ì§€ í™•ì¸
    
    Raises:
        Exception: ë§ˆì§€ë§‰ ì¼ìš”ì¼ì´ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒí•˜ì—¬ DAG ìŠ¤í‚µ
    """
    execution_date = context['execution_date']
    target_date = execution_date.date()
    
    # ì¼ìš”ì¼ì¸ì§€ í™•ì¸ (weekday: 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
    if target_date.weekday() != 6:
        raise Exception(f"{target_date}ëŠ” ì¼ìš”ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. DAG ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # ë‹¤ìŒ ì¼ìš”ì¼ì´ ë‹¤ìŒ ë‹¬ì¸ì§€ í™•ì¸
    next_sunday = target_date + timedelta(days=7)
    if next_sunday.month == target_date.month:
        raise Exception(f"{target_date}ëŠ” í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. DAG ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    logger.info(f"âœ… {target_date}ëŠ” {target_date.strftime('%Yë…„ %mì›”')}ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ìž…ë‹ˆë‹¤.")
    return True


def run_spark_monthly_rebalance(**context):
    """Spark ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± ì‹¤í–‰"""
    logger.info("Starting Spark monthly portfolio rebalancer")
    
    try:
        result = subprocess.run(
            [
                "/opt/spark/bin/spark-submit",
                "--master", "local[2]",
                "--driver-memory", "2g",
                "--executor-memory", "2g",
                "--packages", "org.postgresql:postgresql:42.6.0",
                "/opt/spark-apps/batch/spark_04_monthly_portfolio_rebalancer.py"
            ],
            check=True,
            capture_output=True,
            text=True,
            timeout=900,  # 15 minutes
        )
        
        logger.info("Monthly rebalancer completed successfully")
        logger.info("Output (last 500 chars): %s", result.stdout[-500:] if result.stdout else "")
        return "success"
        
    except subprocess.TimeoutExpired:
        logger.error("Monthly rebalancer timed out after 15 minutes")
        raise
    except subprocess.CalledProcessError as exc:
        logger.error("Monthly rebalancer failed")
        logger.error("stderr (last 1000 chars): %s", exc.stderr[-1000:] if exc.stderr else "")
        raise


def validate_monthly_portfolio(**context):
    """ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„± ê²°ê³¼ ê²€ì¦"""
    import psycopg2
    import os
    
    conn = psycopg2.connect(
        host=os.getenv('POSTGRES_HOST', 'postgres'),
        port=os.getenv('POSTGRES_PORT', '5432'),
        dbname=os.getenv('POSTGRES_DB', 'stockdb'),
        user=os.getenv('POSTGRES_USER', 'postgres'),
        password=os.getenv('POSTGRES_PASSWORD', 'postgres')
    )
    
    cursor = conn.cursor()
    execution_date = context['ds']
    
    # ìƒì„±ëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°œìˆ˜ í™•ì¸
    cursor.execute("""
        SELECT 
            COUNT(*) as stock_count,
            ROUND(SUM(final_weight) * 100, 2) as total_weight
        FROM analytics_08_monthly_portfolio
        WHERE rebalance_date = %s
    """, (execution_date,))
    
    result = cursor.fetchone()
    stock_count, total_weight = result if result else (0, 0)
    
    logger.info(f"ðŸ“Š ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ê²€ì¦ ê²°ê³¼:")
    logger.info(f"  - ì¢…ëª© ìˆ˜: {stock_count}")
    logger.info(f"  - ì´ ê°€ì¤‘ì¹˜: {total_weight}%")
    
    cursor.close()
    conn.close()
    
    # ê²½ê³ ë§Œ ì¶œë ¥ (ì‹¤íŒ¨ì‹œí‚¤ì§€ ì•ŠìŒ)
    if stock_count == 0:
        logger.warning("âš ï¸ í¬íŠ¸í´ë¦¬ì˜¤ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. Spark job í™•ì¸ í•„ìš”.")
    else:
        logger.info("âœ… ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ê²€ì¦ ì™„ë£Œ")


# DAG ì •ì˜
with DAG(
    dag_id='monthly_portfolio_rebalance',
    default_args=default_args,
    description='ë§¤ì›” ë§ˆì§€ë§‰ ì¼ìš”ì¼ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± (5d/10d/20d í†µí•©)',
    schedule_interval='0 14 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ 14:00 UTC
    start_date=datetime(2026, 1, 26),
    catchup=False,
    tags=['monthly', 'portfolio', 'spark', 'rebalance'],
    max_active_runs=1,
) as dag:

    # Task 1: ë§ˆì§€ë§‰ ì¼ìš”ì¼ ì²´í¬
    check_last_sunday = PythonOperator(
        task_id='check_last_sunday',
        python_callable=is_last_sunday_of_month,
        provide_context=True,
    )

    # Task 2: Spark ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹±
    spark_rebalance = PythonOperator(
        task_id='spark_monthly_portfolio_rebalance',
        python_callable=run_spark_monthly_rebalance,
        provide_context=True,
        execution_timeout=timedelta(minutes=20),
    )

    # Task 3: ê²°ê³¼ ê²€ì¦
    validate_portfolio = PythonOperator(
        task_id='validate_monthly_portfolio',
        python_callable=validate_monthly_portfolio,
        provide_context=True,
    )

    # Task Dependencies
    check_last_sunday >> spark_rebalance >> validate_portfolio
