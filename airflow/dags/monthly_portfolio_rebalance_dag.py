"""
Airflow DAG: Monthly Portfolio Rebalance
=========================================
ì‹¤í–‰ ìŠ¤ì¼€ì¤„: ë§¤ì›” ë§ˆì§€ë§‰ ì¼ìš”ì¼ 14:00 UTC (23:00 KST)
ëª©ì : 5ì¼/10ì¼/20ì¼ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í†µí•©í•˜ì—¬ ë‹¤ìŒ 20ì˜ì—…ì¼ ë™ì•ˆ ìœ ì§€í•  ìµœì¢… ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„±

Dependencies:
- benchmark_data_daily_dag (09:00, 10:00 UTC)
- monthly_sector_trending_dag (11:00 UTC)
- etf_holdings_daily_dag (12:00 UTC)
- Spark job (13:00 UTC)
â†’ ëª¨ë“  ì¼ì¼ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ ì‹¤í–‰

Output:
- analytics_monthly_portfolio í…Œì´ë¸”
"""

from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import calendar

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def is_last_sunday_of_month(**context):
    """
    ì˜¤ëŠ˜ì´ í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì¸ì§€ í™•ì¸
    
    Returns:
        bool: Trueë©´ ê³„ì† ì§„í–‰, Falseë©´ DAG ì¤‘ë‹¨
    
    Raises:
        Exception: ë§ˆì§€ë§‰ ì¼ìš”ì¼ì´ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒí•˜ì—¬ DAG ìŠ¤í‚µ
    """
    execution_date = context['execution_date']
    target_date = execution_date.date()
    
    # ì¼ìš”ì¼ì¸ì§€ í™•ì¸ (weekday: 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
    if target_date.weekday() != 6:
        raise Exception(f"{target_date}ëŠ” ì¼ìš”ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. DAG ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # ë‹¤ìŒ ì¼ìš”ì¼ì´ ë‹¤ìŒ ë‹¬ì¸ì§€ í™•ì¸
    next_sunday = target_date + timedelta(days=7)
    if next_sunday.month == target_date.month:
        raise Exception(f"{target_date}ëŠ” í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. DAG ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print(f"âœ… {target_date}ëŠ” {target_date.strftime('%Yë…„ %mì›”')}ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì…ë‹ˆë‹¤. ë¦¬ë°¸ëŸ°ì‹±ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    return True


# DAG ì •ì˜
with DAG(
    dag_id='monthly_portfolio_rebalance',
    default_args=default_args,
    description='ë§¤ì›” ë§ˆì§€ë§‰ ì¼ìš”ì¼ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± (5d/10d/20d í†µí•©)',
    schedule_interval='0 14 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ 14:00 UTC (23:00 KST) - ì‹¤ì œë¡œëŠ” ë§ˆì§€ë§‰ ì¼ìš”ì¼ë§Œ ì‹¤í–‰
    start_date=days_ago(1),
    catchup=False,
    tags=['monthly', 'portfolio', 'spark', 'rebalance'],
    max_active_runs=1,
) as dag:

    # Task 1: ë§ˆì§€ë§‰ ì¼ìš”ì¼ ì²´í¬
    check_last_sunday = PythonOperator(
        task_id='check_last_sunday',
        python_callable=is_last_sunday_of_month,
        provide_context=True,
    )

    # Task 2: Spark ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹±
    spark_monthly_rebalance = SparkSubmitOperator(
        task_id='spark_monthly_portfolio_rebalance',
        application='/opt/spark-apps/batch/spark_monthly_portfolio_rebalancer.py',
        conn_id='spark_default',
        conf={
            'spark.master': 'spark://spark-master:7077',
            'spark.executor.memory': '2g',
            'spark.driver.memory': '1g',
            'spark.executor.cores': '2',
            'spark.sql.adaptive.enabled': 'true',
            'spark.jars': '/opt/spark/jars/postgresql-42.7.4.jar',
        },
        application_args=['{{ ds }}'],  # as_of_date ì „ë‹¬
        verbose=True,
        driver_class_path='/opt/spark/jars/postgresql-42.7.4.jar',
    )

    # Task 3: ê²°ê³¼ ê²€ì¦ (ì˜µì…˜)
    def validate_monthly_portfolio(**context):
        """ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„± ê²°ê³¼ ê²€ì¦"""
        import psycopg2
        import os
        
        conn = psycopg2.connect(
            host=os.getenv('POSTGRES_HOST', 'postgres'),
            port=os.getenv('POSTGRES_PORT', '5432'),
            dbname=os.getenv('POSTGRES_DB', 'finviz_stock_db'),
            user=os.getenv('POSTGRES_USER', 'finvizuser'),
            password=os.getenv('POSTGRES_PASSWORD', 'finvizpass')
        )
        
        cursor = conn.cursor()
        execution_date = context['ds']
        
        # ìƒì„±ëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°œìˆ˜ í™•ì¸
        cursor.execute("""
            SELECT 
                COUNT(*) as stock_count,
                ROUND(SUM(final_weight) * 100, 2) as total_weight,
                MIN(final_rank) as min_rank,
                MAX(final_rank) as max_rank
            FROM analytics_monthly_portfolio
            WHERE rebalance_date = %s
        """, (execution_date,))
        
        result = cursor.fetchone()
        stock_count, total_weight, min_rank, max_rank = result
        
        print(f"ğŸ“Š ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ê²€ì¦ ê²°ê³¼:")
        print(f"  - ì¢…ëª© ìˆ˜: {stock_count}")
        print(f"  - ì´ ê°€ì¤‘ì¹˜: {total_weight}%")
        print(f"  - ìˆœìœ„ ë²”ìœ„: {min_rank} ~ {max_rank}")
        
        # ê²€ì¦: ì¢…ëª© ìˆ˜ê°€ 10ê°œ ì´ìƒ, ì´ ê°€ì¤‘ì¹˜ 95% ì´ìƒ
        if stock_count < 10:
            raise Exception(f"âŒ ì¢…ëª© ìˆ˜ ë¶€ì¡±: {stock_count}ê°œ (ìµœì†Œ 10ê°œ í•„ìš”)")
        
        if total_weight < 95.0:
            raise Exception(f"âŒ ì´ ê°€ì¤‘ì¹˜ ë¶€ì¡±: {total_weight}% (ìµœì†Œ 95% í•„ìš”)")
        
        # ìƒìœ„ 10ê°œ ì¢…ëª© ì¶œë ¥
        cursor.execute("""
            SELECT final_rank, ticker, company_name, 
                   ROUND(final_weight * 100, 2) as weight_pct,
                   score, source_periods
            FROM analytics_monthly_portfolio
            WHERE rebalance_date = %s
            ORDER BY final_rank
            LIMIT 10
        """, (execution_date,))
        
        print("\nğŸ“ˆ ìƒìœ„ 10ê°œ ì¢…ëª©:")
        for row in cursor.fetchall():
            rank, ticker, name, weight, score, periods = row
            print(f"  {rank:2d}. {ticker:5s} | {name:30s} | {weight:5.2f}% | score={score:.1f} | {periods}")
        
        cursor.close()
        conn.close()
        
        print("\nâœ… ì›”ê°„ í¬íŠ¸í´ë¦¬ì˜¤ ê²€ì¦ ì™„ë£Œ")
    
    validate_portfolio = PythonOperator(
        task_id='validate_monthly_portfolio',
        python_callable=validate_monthly_portfolio,
        provide_context=True,
    )

    # Task Dependencies
    check_last_sunday >> spark_monthly_rebalance >> validate_portfolio
